{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b388447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: figure out how to get saliency for transformer\n",
    "# can we match CLIP performance with VB?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a0dce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmf.utils.env import import_user_module\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import_user_module(\"/home/g-luo/remote_home/foil_mmf\")\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torchray.attribution.grad_cam import grad_cam\n",
    "from miniclip.imageWrangle import heatmap, heatmap_helper, min_max_norm, torch_to_rgba\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "def get_feat(ann):\n",
    "    base = \"/shared/g-luo/visual_news_metadata/features/\"\n",
    "    rest = ann[\"image_path\"].replace(\"./\", \"\").replace(\".jpg\", \"_info.npy\").replace(\"images\", \"features\")\n",
    "    return base + rest\n",
    "\n",
    "def get_categories():\n",
    "    categories_unprocessed = json.load(open(\"../foil_mmf/etc/visual_genome_categories.json\"))\n",
    "    categories = {cat[\"id\"]: cat[\"name\"] for cat in categories_unprocessed[\"categories\"]}\n",
    "    return categories\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "label_categories = get_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# sys.path.append(\"../foil_mmf/foil/models\")\n",
    "from natural_language_joint_query_search.CLIP import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b37030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "rn50_dir = \"/shared/g-luo/tarasque/emnlp/RN50_freeze_lower/merged_balanced_clip_rn50_freeze_lower/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d8ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = json.load(open(\"../foil_mmf/val/merged_balanced_clip_rn50_freeze_lower/foil_clip_2/reports/val.json\"))\n",
    "predictions = {ann[\"id\"]:ann for ann in predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f748521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import OrderedDict\n",
    "\n",
    "device = \"cuda\"\n",
    "def get_model(load_weights=True, model_dir=\"\"):\n",
    "    model, preprocess =  clip.load(\"RN50\", device=device, jit=False)\n",
    "    if load_weights:\n",
    "        print(\"Loaded\")\n",
    "        state_dict = torch.load(f\"{model_dir}\")\n",
    "        lst = []\n",
    "        for key in state_dict[\"model\"]:\n",
    "            new_key = key.replace(\"model.\", \"\")\n",
    "            if \"classifier\" not in new_key:\n",
    "                lst.append((new_key, state_dict[\"model\"][key]))\n",
    "        model.load_state_dict(OrderedDict(lst))\n",
    "        model.to(\"cuda\")\n",
    "        model.eval()\n",
    "    return model, preprocess\n",
    "\n",
    "finetuned_model, preprocess = get_model(load_weights=True, model_dir=rn50_dir + \"best.ckpt\")\n",
    "finetuned_model = finetuned_model.float()\n",
    "\n",
    "original_model, _ = get_model(load_weights=False)\n",
    "original_model = original_model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf747b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from foil_mmf.foil.models.clip import CLIP\n",
    "# from mmf.utils.configuration import load_yaml_with_defaults\n",
    "\n",
    "# conf = load_yaml_with_defaults(rn50_dir + \"config.yaml\")\n",
    "# classification_model = CLIP(conf.model_config.clip)\n",
    "# classification_model.build()\n",
    "\n",
    "# state_dict = torch.load(rn50_dir + \"/best.ckpt\")\n",
    "# classification_model.load_state_dict(state_dict[\"model\"])\n",
    "# classification_model.to(\"cuda\")\n",
    "# classification_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d6c04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "option = \"merged/balanced_disjoint_ents_quad\"\n",
    "dataset_type = \"val\"\n",
    "dataset = json.load(open(f\"/home/g-luo/.cache/torch/mmf/data/datasets/foil/defaults/annotations/visualnews_foil/{option}/{dataset_type}.json\"))[\"annotations\"]\n",
    "metadata = json.load(open(f\"/shared/g-luo/visual_news_metadata/visualnews_foil/full_split/data/{dataset_type}.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f6caf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = {ann[\"id\"]:ann[\"caption_entities\"] for ann in metadata}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97412e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/CLIP/issues/18\n",
    "# TODO: figure out saliency for ViT-B32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea87ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.5, 0.7, 0.8][0]\n",
    "layer = 'layer4.2.relu'\n",
    "def get_grad_cam(model, image, text):\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features, _ = model.encode_text(text)\n",
    "        image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
    "        image_features_new = image_features / image_features_norm\n",
    "        text_features_norm = text_features.norm(dim=-1, keepdim=True)\n",
    "        text_features_new = text_features / text_features_norm\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features_new @ text_features_new.t()\n",
    "        probs = logits_per_image.softmax(dim=-1).cpu().numpy().tolist()\n",
    "\n",
    "    saliency = grad_cam(model.visual, image.type(model.dtype), image_features, saliency_layer=layer)\n",
    "    hm = heatmap(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)\n",
    "\n",
    "    collect_images, saliencies = [], []\n",
    "    for i in range(len(categories)):\n",
    "        # mutliply the normalized text embedding with image norm to get approx image embedding\n",
    "        text_prediction = (text_features_new[[i]] * image_features_norm)\n",
    "        saliency = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\n",
    "        hm_helper = heatmap_helper(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)\n",
    "        hm = heatmap(image[0], saliency[0][0,].detach().type(torch.float32).cpu(), alpha=alpha)\n",
    "        collect_images.append(hm)\n",
    "        saliencies.append(hm_helper)\n",
    "    logits = logits_per_image.cpu().numpy().tolist()[0]\n",
    "    titles = [f\"{x} - {str(round(y, 3))}/{str(round(l, 2))}\" for (x, y, l) in zip(categories, probs[0], logits)]\n",
    "    return titles, collect_images, saliencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "import torch, os\n",
    "\n",
    "# get the proportion of times that the attn lands in each bbox\n",
    "def get_attention_bbox(ann, saliency, threshold=0.5):\n",
    "    saliency = saliency / np.sum(saliency)\n",
    "    img_info = np.load(get_feat(ann), allow_pickle=True).item()\n",
    "    img_path = \"/shared/g-luo/visual_news/origin/\" + ann[\"image_path\"].split(\"./\")[1]\n",
    "    img = Image.open(img_path)\n",
    "    img_array = torch_to_rgba(image[0])\n",
    "\n",
    "    # bboxes have no need for normalization\n",
    "    weights = []\n",
    "    for i in range(100):\n",
    "        if img_info[\"cls_prob\"][i].max() < threshold:\n",
    "            continue\n",
    "        w, h = img_info[\"image_width\"], img_info[\"image_height\"]\n",
    "        x1, y1, x2, y2 = img_info[\"bbox\"][i]\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        label_idx = np.argmax(img_info[\"cls_prob\"][i])\n",
    "        label = label_categories[label_idx]\n",
    "        \n",
    "        weights.append((label, np.sum(saliency[x1:x2, y1:y2])))\n",
    "    print(max(weights, key=lambda x:x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9589ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from https://github.com/pytorch/captum/blob/master/captum/attr/_utils/visualization.py\n",
    "from IPython.core.display import HTML, display\n",
    "def _get_color(attr):\n",
    "    # clip values to prevent CSS errors (Values should be from [-1,1])\n",
    "    attr = max(-1, min(1, attr))\n",
    "    if attr > 0:\n",
    "        hue = 120\n",
    "        sat = 75\n",
    "        lig = 100 - int(50 * attr)\n",
    "    else:\n",
    "        hue = 0\n",
    "        sat = 75\n",
    "        lig = 100 - int(-40 * attr)\n",
    "    return \"hsl({}, {}%, {}%)\".format(hue, sat, lig)\n",
    "\n",
    "def format_special_tokens(token):\n",
    "    if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "        return \"#\" + token.strip(\"<>\")\n",
    "    return token\n",
    "\n",
    "def format_word_importances(words, importances):\n",
    "    if importances is None or len(importances) == 0:\n",
    "        return \"<td></td>\"\n",
    "    assert len(words) <= len(importances)\n",
    "    tags = [\"<td>\"]\n",
    "    for word, importance in zip(words, importances[: len(words)]):\n",
    "        word = format_special_tokens(word)\n",
    "        color = _get_color(importance)\n",
    "        unwrapped_tag = '<mark style=\"background-color: {color}; opacity:1.0; \\\n",
    "                    line-height:1.75\"><font color=\"black\"> {word}\\\n",
    "                    </font></mark>'.format(\n",
    "            color=color, word=word\n",
    "        )\n",
    "        tags.append(unwrapped_tag)\n",
    "    tags.append(\"</td>\")\n",
    "    return \"\".join(tags)\n",
    "\n",
    "def visualize_text(\n",
    "    datarecords, title\n",
    ") -> \"HTML\":  # In quotes because this type doesn't exist in standalone mode\n",
    "    dom = [\"<table width: 100%>\"]\n",
    "    rows = [\n",
    "        \"<th>\" + title + \"</th>\"\n",
    "    ]\n",
    "    for datarecord in datarecords:\n",
    "        rows.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    \"<tr>\",\n",
    "                    format_word_importances(\n",
    "                        datarecord.raw_input, datarecord.word_attributions\n",
    "                    ),\n",
    "                    \"<tr>\",\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    dom.append(\"\".join(rows))\n",
    "    dom.append(\"</table>\")\n",
    "    html = HTML(\"\".join(dom))\n",
    "    display(html)\n",
    "\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea75455",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from captum.attr import visualization\n",
    "def get_text_scores(model, image, ann, title):\n",
    "    words = ann[\"caption\"].split(\" \")\n",
    "    text = clip.tokenize(ann[\"caption\"]).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features, weight = model.encode_text(text)\n",
    "        \n",
    "    attention_weights = list(weight[-1][0][1+len(words)].cpu().numpy())[:2+len(words)][1:][:-1]\n",
    "    attention_weights = [float(item) for item in attention_weights]\n",
    "#     scores = (image_features @ text_features.T).cpu().numpy()[0]\n",
    "#     scores = scores / np.sum(scores)\n",
    "#     #words = np.expand_dims(np.array(words), 0)\n",
    "#     print(scores)\n",
    "    vis_data_records = [visualization.VisualizationDataRecord(attention_weights,0,0,0,0,0,words,1)]\n",
    "    visualize_text(vis_data_records, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592dda9b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(20, 50):\n",
    "    ann = dataset[i]\n",
    "    print(ann[\"id\"])\n",
    "    print(\"Caption: \", ann[\"caption\"])\n",
    "    print(\"Image Caption: \", ann[\"image_caption\"])\n",
    "    print(\"\")\n",
    "    base_path = \"/shared/g-luo/visual_news/origin/\"\n",
    "    imageFile = base_path + ann[\"image_path\"].replace(\"./\", \"\")\n",
    "#     categories = [f\"an image of {ent[0]}\" for ent in ents[ann[\"id\"]]]\n",
    "    categories = [ann[\"caption\"]]\n",
    "    if imageFile:\n",
    "        image_raw = Image.open(imageFile)\n",
    "        # preprocess image:\n",
    "        image = preprocess(image_raw).unsqueeze(0).to(device)\n",
    "        # preprocess text\n",
    "        # categories = [ann[\"caption\"]]\n",
    "        text = clip.tokenize(categories).to(device)\n",
    "        \n",
    "        get_text_scores(finetuned_model, image, ann, \"Finetuned Word Importance\")\n",
    "        get_text_scores(original_model, image, ann, \"Original Word Importance\")\n",
    "\n",
    "        finetuned_titles, finetuned_collect_images, finetuned_saliencies = get_grad_cam(finetuned_model, image, text)\n",
    "        original_titles, original_collect_images, original_saliences = get_grad_cam(original_model, image, text)\n",
    "        \n",
    "        if i % 2 == 0:\n",
    "            predicted = predictions[ann[\"id\"]]\n",
    "        else: \n",
    "            predicted = predictions[ann[\"id\"] * -1]\n",
    "            \n",
    "        for i in range(len(finetuned_collect_images)):\n",
    "            f, ax = plt.subplots(1,2)\n",
    "            # print(\"FINETUNED, FREEZE LOWER \", finetuned_titles[i])\n",
    "            print(\"Target \", predicted[\"target\"], \"Prediction \", predicted[\"score\"])\n",
    "            ax[0].imshow(finetuned_collect_images[i])\n",
    "            print(\" \")\n",
    "            # print(\"ORIGINAL \",original_titles[i])\n",
    "            ax[1].imshow(original_collect_images[i])\n",
    "            plt.show()\n",
    "            \n",
    "        # get_attention_bbox(ann, finetuned_saliencies[0])\n",
    "        print(\"===================================\")\n",
    "\n",
    "    #     print(\"Original Image and Grad Cam for image embedding\")\n",
    "    #     original, gradcam = Image.fromarray((torch_to_rgba(image[0]).numpy() * 255.).astype(np.uint8)), hm\n",
    "    #     fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 10))\n",
    "    #     axes[0].imshow(original)\n",
    "    #     axes[1].imshow(gradcam)\n",
    "    #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b590b125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
